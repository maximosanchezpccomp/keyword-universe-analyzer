"""
Servicio para integraci√≥n con OpenAI API con sistema de cach√©
"""

from openai import OpenAI
import pandas as pd
import json
from typing import Dict, List, Any
import logging

# Importar cache manager
from app.utils.cache_manager import get_cache_manager
from config import CACHE_CONFIG, estimate_analysis_cost

logger = logging.getLogger(__name__)


class OpenAIService:
    """Servicio para interactuar con la API de OpenAI con cach√© inteligente"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o"):
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.max_tokens = 16000 if model in ["gpt-4o", "gpt-4-turbo"] else 4096
        
        # Cache manager
        self.cache_manager = get_cache_manager()
        self.cache_enabled = CACHE_CONFIG.get('enabled', True)
        
        logger.info(f"OpenAIService inicializado - Modelo: {model}, Cach√©: {self.cache_enabled}")
    
    def create_universe_prompt(
        self,
        df: pd.DataFrame,
        analysis_type: str = "Tem√°tica (Topics)",
        num_tiers: int = 3,
        custom_instructions: str = "",
        include_semantic: bool = True,
        include_trends: bool = True,
        include_gaps: bool = True
    ) -> List[Dict[str, str]]:
        """Crea los mensajes para OpenAI en formato chat"""
        
        # Preparar datos
        top_keywords = df.nlargest(1000, 'volume')[['keyword', 'volume', 'traffic']].to_dict('records')
        
        stats = {
            'total_keywords': len(df),
            'total_volume': int(df['volume'].sum()),
            'avg_volume': int(df['volume'].mean()),
            'unique_keywords': df['keyword'].nunique()
        }
        
        # Determinar tipo de an√°lisis
        analysis_instructions = self._get_analysis_instructions(analysis_type)
        
        # Secciones opcionales
        gaps_section = ""
        if include_gaps:
            gaps_section = """,
    "gaps": [
        {
            "topic": "Nombre del gap/oportunidad",
            "volume": 50000,
            "keyword_count": 25,
            "description": "Por qu√© es una oportunidad",
            "difficulty": "low|medium|high"
        }
    ]"""
        
        trends_section = ""
        if include_trends:
            trends_section = """,
    "trends": [
        {
            "trend": "Nombre de la tendencia",
            "keywords": ["keyword1", "keyword2"],
            "total_volume": 50000,
            "insight": "Por qu√© es relevante"
        }
    ]"""
        
        system_message = """Eres un experto en SEO y an√°lisis estrat√©gico de keywords. 
Tu especialidad es identificar patrones, oportunidades y crear estrategias data-driven.
Siempre respondes con JSON v√°lido y an√°lisis profundos."""
        
        user_message = f"""Analiza las siguientes keywords y crea un "Keyword Universe" completo.

# CONTEXTO
- Total keywords: {stats['total_keywords']:,}
- Volumen total: {stats['total_volume']:,}
- Volumen promedio: {stats['avg_volume']:,}
- Keywords √∫nicas: {stats['unique_keywords']:,}

# TIPO DE AN√ÅLISIS
{analysis_type}

{analysis_instructions}

# TU TAREA
Crea un an√°lisis con {num_tiers} tiers (niveles) de prioridad:
- Tier 1: Alto volumen y m√°xima prioridad estrat√©gica
- Tier {num_tiers}: Menor volumen pero oportunidades espec√≠ficas

{custom_instructions}

{"IMPORTANTE: Realiza an√°lisis sem√°ntico profundo para entender la intenci√≥n real detr√°s de cada keyword." if include_semantic else ""}
{"IMPORTANTE: Identifica tendencias emergentes y keywords en crecimiento." if include_trends else ""}
{"IMPORTANTE: Detecta gaps de contenido - topics con alto volumen pero poca cobertura competitiva." if include_gaps else ""}

# FORMATO DE RESPUESTA (CR√çTICO - RESPONDER SOLO CON JSON)
Responde √öNICAMENTE con un JSON v√°lido con esta estructura:

{{
    "summary": "Resumen ejecutivo del an√°lisis en 3-4 p√°rrafos explicando hallazgos clave, oportunidades principales y recomendaciones estrat√©gicas",
    "topics": [
        {{
            "topic": "Nombre descriptivo del topic",
            "tier": 1,
            "keyword_count": 50,
            "volume": 150000,
            "traffic": 45000,
            "priority": "high|medium|low",
            "description": "Descripci√≥n detallada: por qu√© es importante, qu√© representa, estrategia recomendada",
            "example_keywords": ["keyword1", "keyword2", "keyword3", "keyword4", "keyword5"]
        }}
    ]{gaps_section}{trends_section}
}}

# KEYWORDS A ANALIZAR
{json.dumps(top_keywords[:1000], indent=2)}

CR√çTICO: Responde SOLO con el JSON v√°lido, sin texto adicional antes o despu√©s."""

        return [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ]
    
    def _get_analysis_instructions(self, analysis_type: str) -> str:
        """Retorna instrucciones espec√≠ficas seg√∫n el tipo de an√°lisis"""
        
        instructions = {
            "Tem√°tica (Topics)": """
Agrupa keywords por temas sem√°nticos coherentes. Cada topic debe representar 
un √°rea tem√°tica clara. Busca patrones de co-ocurrencia y relevancia sem√°ntica.""",
            
            "Intenci√≥n de b√∫squeda": """
Clasifica keywords seg√∫n la intenci√≥n del usuario:
- Informacional: Busca aprender (c√≥mo, qu√© es, gu√≠a)
- Navegacional: Busca un sitio espec√≠fico (login, descargar, marca)
- Comercial: Investiga antes de comprar (mejor, review, comparar)
- Transaccional: Listo para actuar (comprar, precio, gratis)

Dentro de cada intenci√≥n, agrupa por sub-temas.""",
            
            "Funnel de conversi√≥n": """
Clasifica keywords seg√∫n la etapa del funnel:
- TOFU (Top): Awareness - descubrimiento del problema
- MOFU (Middle): Consideration - evaluaci√≥n de soluciones  
- BOFU (Bottom): Decision - listo para decidir

Asigna tiers considerando el valor estrat√©gico de cada etapa."""
        }
        
        return instructions.get(analysis_type, instructions["Tem√°tica (Topics)"])
    
    def analyze_keywords(
        self,
        messages: List[Dict[str, str]],
        df: pd.DataFrame,
        use_cache: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Env√≠a el prompt a OpenAI y procesa la respuesta con sistema de cach√©
        
        Args:
            messages: Mensajes en formato chat
            df: DataFrame con keywords
            use_cache: Si usar el sistema de cach√©
            **kwargs: Par√°metros adicionales del an√°lisis
        """
        
        # Generar hash para cach√©
        cache_hash = None
        if self.cache_enabled and use_cache:
            cache_hash = self.cache_manager.generate_hash(
                df=df,
                analysis_type=kwargs.get('analysis_type', 'Tem√°tica (Topics)'),
                num_tiers=kwargs.get('num_tiers', 3),
                custom_instructions=kwargs.get('custom_instructions', ''),
                include_semantic=kwargs.get('include_semantic', True),
                include_trends=kwargs.get('include_trends', True),
                include_gaps=kwargs.get('include_gaps', True)
            )
            
            logger.info(f"Cache hash generado: {cache_hash}")
            
            # Intentar recuperar del cach√©
            ttl_hours = CACHE_CONFIG.get('default_ttl_hours', 24)
            cached_result = self.cache_manager.get_cached_analysis(cache_hash, ttl_hours)
            
            if cached_result is not None:
                logger.info(f"‚úÖ Resultado recuperado del cach√© (ahorro de ${cached_result.get('_cache_metadata', {}).get('cost', 0)})")
                return cached_result
            
            logger.info("‚ùå Cache miss - Realizando an√°lisis nuevo")
        
        # Estimar costo
        cost_estimate = estimate_analysis_cost(self.model, len(df))
        logger.info(f"Costo estimado: ${cost_estimate['cost']}")
        
        try:
            # Llamada real a la API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=self.max_tokens,
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            # Extraer contenido
            response_text = response.choices[0].message.content
            
            # Parsear JSON
            try:
                result = json.loads(response_text)
            except json.JSONDecodeError:
                # Intentar extraer JSON del texto
                import re
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                else:
                    raise ValueError("No se pudo extraer JSON v√°lido de la respuesta")
            
            # Enriquecer resultados
            result = self._enrich_results(result, df)
            
            # Guardar en cach√© si est√° habilitado
            if self.cache_enabled and use_cache and cache_hash:
                saved = self.cache_manager.save_analysis(
                    cache_hash=cache_hash,
                    result=result,
                    provider='OpenAI',
                    model=self.model,
                    estimated_cost=cost_estimate['cost'],
                    estimated_credits=cost_estimate['input_tokens'] + cost_estimate['output_tokens'],
                    parameters=kwargs
                )
                
                if saved:
                    logger.info(f"üíæ An√°lisis guardado en cach√©: {cache_hash}")
            
            return result
            
        except Exception as e:
            logger.error(f"Error al analizar con OpenAI: {str(e)}")
            raise Exception(f"Error al analizar con OpenAI: {str(e)}")
    
    def _enrich_results(self, result: Dict, df: pd.DataFrame) -> Dict:
        """Enriquece los resultados con datos adicionales"""
        
        if 'topics' in result:
            for topic in result['topics']:
                # Asegurar tipos correctos
                topic['keyword_count'] = int(topic.get('keyword_count', 0))
                topic['volume'] = int(topic.get('volume', 0))
                topic['traffic'] = int(topic.get('traffic', 0))
                
                # Calcular m√©tricas adicionales
                if topic['keyword_count'] > 0:
                    topic['avg_volume_per_keyword'] = topic['volume'] / topic['keyword_count']
                else:
                    topic['avg_volume_per_keyword'] = 0
                
                # Asegurar que tier sea int
                topic['tier'] = int(topic.get('tier', 1))
        
        return result
    
    def compare_with_claude(
        self, 
        claude_result: Dict[str, Any], 
        df: pd.DataFrame
    ) -> Dict[str, Any]:
        """
        Compara y valida resultados de Claude con an√°lisis de OpenAI
        """
        
        messages = [
            {
                "role": "system",
                "content": """Eres un experto en SEO que valida y complementa an√°lisis de keywords.
Tu rol es identificar fortalezas, debilidades y oportunidades perdidas en an√°lisis existentes."""
            },
            {
                "role": "user",
                "content": f"""Revisa este an√°lisis de keywords y proporciona:

1. **Validaci√≥n general**: ¬øEs completo y preciso el an√°lisis?
2. **Topics perdidos**: ¬øHay temas importantes que no se identificaron?
3. **Mejoras sugeridas**: ¬øC√≥mo mejorar la clasificaci√≥n o priorizaci√≥n?

AN√ÅLISIS A REVISAR:
{json.dumps(claude_result.get('topics', [])[:20], indent=2)}

KEYWORDS DISPONIBLES (muestra):
{json.dumps(df.nlargest(200, 'volume')[['keyword', 'volume']].to_dict('records'), indent=2)}

Responde en JSON:
{{
    "validation": "Evaluaci√≥n general del an√°lisis (2-3 p√°rrafos)",
    "missing_topics": ["topic1 no identificado", "topic2 no identificado"],
    "improvements": ["mejora sugerida 1", "mejora sugerida 2"],
    "agreement_score": 85,
    "key_differences": "Principales diferencias con tu an√°lisis"
}}"""
            }
        ]
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=2000,
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"Error en comparaci√≥n: {str(e)}")
            return {
                "validation": "No se pudo completar la validaci√≥n cruzada",
                "missing_topics": [],
                "improvements": [],
                "error": str(e)
            }
    
    def get_topic_details(self, topic_name: str, df: pd.DataFrame) -> pd.DataFrame:
        """Identifica keywords espec√≠ficas que pertenecen a un topic"""
        
        sample_keywords = df.nlargest(500, 'volume')[['keyword', 'volume']].to_dict('records')
        
        messages = [
            {
                "role": "system",
                "content": "Eres un experto en clasificaci√≥n sem√°ntica de keywords."
            },
            {
                "role": "user",
                "content": f"""Dado el topic "{topic_name}", identifica qu√© keywords del siguiente 
dataset pertenecen a este topic.

Responde SOLO con JSON:
{{
    "keywords": ["keyword1", "keyword2", ...]
}}

Dataset:
{json.dumps(sample_keywords, indent=2)}"""
            }
        ]
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=2000,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            matching_keywords = result.get('keywords', [])
            
            return df[df['keyword'].isin(matching_keywords)]
            
        except Exception as e:
            logger.error(f"Error obteniendo detalles del topic: {str(e)}")
            return pd.DataFrame()
